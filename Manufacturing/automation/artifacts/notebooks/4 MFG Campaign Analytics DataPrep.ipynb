{
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "sessionKeepAliveTimeout": 30
  },
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Please don't run / don't click \"Run all\" the notebook:\n",
        "At the time of writing of this document, the current core limit is 200 cores per workspace and depending upon number of concurrent users, you may end up with core capacity being exceeded or maximum number of parallel jobs being exceeded error. \n",
        "## Fetch Marketing Campaigns data into DataFrame and Calculate Revenue Variance        "
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "outputs": [
        {
          "execution_count": 3,
          "output_type": "execute_result",
          "data": {
            "text/plain": "+--------------------+---------+--------------------+--------------------+----------+--------------+\n|              Region|  Country|    Product_Category|       Campaign_Name|   Revenue|Revenue_Target|\n+--------------------+---------+--------------------+--------------------+----------+--------------+\n|North & Central A...|   Canada|               Books|EnjoyTheMoment; B...|$13,873.00|    $10,617.00|\n|              Europe|  Germany|Apparel and Footwear|     Fun with Colors|$14,865.00|    $15,960.00|\n|       South America|   Brazil|               Books|EnjoyTheMoment; B...|$16,611.00|     $7,917.00|\n|        Asia Pacific|      USA|               Books|EnjoyTheMoment; B...|$12,174.00|     $6,996.00|\n|              Europe|    Italy|               Books|EnjoyTheMoment; B...| $5,867.00|    $19,049.00|\n|North & Central A...|Australia|               Books|EnjoyTheMoment; B...| $9,112.00|    $11,930.00|\n|       South America|   Canada|               Books|EnjoyTheMoment; B...|$16,386.00|     $9,989.00|\n|        Asia Pacific|   Brazil|               Books|EnjoyTheMoment; B...|$14,691.00|    $14,601.00|\n|              Europe|Indonesia|               Books|EnjoyTheMoment; B...|$12,974.00|    $19,462.00|\n|North & Central A...|Indonesia|               Books|EnjoyTheMoment; B...|$10,246.00|    $18,683.00|\n+--------------------+---------+--------------------+--------------------+----------+--------------+\nonly showing top 10 rows"
          },
          "metadata": {}
        }
      ],
      "metadata": {},
      "source": [
        "%%pyspark\n",
        "data_path = spark.read.load('abfss://marketingdb-staging@#DATA_LAKE_NAME#.dfs.core.windows.net/CampaignAnalytics.csv', format='csv',header=True)\n",
        "data_path.show(10)"
      ],
      "attachments": {}
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load into Pandas and Perform Cleansing Operations\n",
        ""
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {},
      "source": [
        "%%pyspark\n",
        "from pyspark.sql.functions import *\n",
        "from pyspark.sql.types import *\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "pd_df = data_path.select(\"*\").toPandas()\n",
        "\n",
        "'''Cleansing Operations: \n",
        "1. Columns Revenue, Revenue_Target: Remove '$' symbol and convert datatype to float\n",
        "2. Columns Revenue, Revenue_Target: Replace null values with 0\n",
        "3. Columns Region, Country, Product_Category, Campaign_Name: Convert columns to Camel Case\n",
        "'''\n",
        "pd_df['Revenue']= pd_df['Revenue'].replace('[\\$,]', '', regex=True).astype(float)\n",
        "pd_df['Revenue_Target']= pd_df['Revenue_Target'].replace('[\\$,]', '', regex=True).astype(float)\n",
        "pd_df['Revenue'].fillna(value=0, inplace=True)\n",
        "pd_df['Revenue_Target'].fillna(value=0, inplace=True)\n",
        "\n",
        "pd_df['Region'] = pd_df.Region.str.title()\n",
        "pd_df['Country'] = pd_df.Country.str.title()\n",
        "pd_df['Product_Category'] = pd_df.Product_Category.str.title()\n",
        "pd_df['Campaign_Name'] = pd_df.Campaign_Name.str.title()"
      ],
      "attachments": {}
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data Transformation - Calculate Revenue Variance\n",
        ""
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "outputs": [
        {
          "execution_count": 4,
          "output_type": "execute_result",
          "data": {
            "text/plain": "Region  Country       ...        Revenue_Target Revenue_Variance\n1         Europe  Germany       ...               15960.0           1095.0\n2  South America   Brazil       ...                7917.0          -8694.0\n3   Asia Pacific      USA       ...                6996.0          -5178.0\n4         Europe    Italy       ...               19049.0          13182.0\n\n[4 rows x 7 columns]"
          },
          "metadata": {}
        }
      ],
      "metadata": {},
      "source": [
        "#Create new column\n",
        "pd_df['Revenue_Variance'] = pd_df['Revenue_Target'] - pd_df['Revenue']\n",
        "\n",
        "print(pd_df[1:5])"
      ],
      "attachments": {}
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Move data to Azure Data Lake Gen2\n",
        ""
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "outputs": [
        {
          "execution_count": 5,
          "output_type": "execute_result",
          "data": {
            "text/plain": "+--------------------+-------+--------------------+--------------------+-------+--------------+----------------+\n|              Region|Country|    Product_Category|       Campaign_Name|Revenue|Revenue_Target|Revenue_Variance|\n+--------------------+-------+--------------------+--------------------+-------+--------------+----------------+\n|North & Central A...| Canada|               Books|EnjoyTheMoment; B...|13873.0|       10617.0|         -3256.0|\n|              Europe|Germany|Apparel and Footwear|     Fun with Colors|14865.0|       15960.0|          1095.0|\n|       South America| Brazil|               Books|EnjoyTheMoment; B...|16611.0|        7917.0|         -8694.0|\n|        Asia Pacific|    USA|               Books|EnjoyTheMoment; B...|12174.0|        6996.0|         -5178.0|\n|              Europe|  Italy|               Books|EnjoyTheMoment; B...| 5867.0|       19049.0|         13182.0|\n+--------------------+-------+--------------------+--------------------+-------+--------------+----------------+\nonly showing top 5 rows\n\n/opt/spark/python/lib/pyspark.zip/pyspark/sql/session.py:714: UserWarning: createDataFrame attempted Arrow optimization because 'spark.sql.execution.arrow.enabled' is set to true; however, failed by the reason below:\n  'JavaPackage' object is not callable\nAttempting non-optimization as 'spark.sql.execution.arrow.fallback.enabled' is set to true."
          },
          "metadata": {}
        }
      ],
      "metadata": {},
      "source": [
        "%%pyspark\n",
        "df = spark.createDataFrame(pd_df)\n",
        "df.show(5)\n",
        "\n",
        "(df\n",
        " .coalesce(1)\n",
        " .write\n",
        " .mode(\"overwrite\")\n",
        " .option(\"header\", \"true\")\n",
        " .format(\"com.databricks.spark.csv\")\n",
        " .save('abfss://marketingdb-staging@#DATA_LAKE_NAME#.dfs.core.windows.net/Campaignsdata'))"
      ],
      "attachments": {}
    }
  ]
}