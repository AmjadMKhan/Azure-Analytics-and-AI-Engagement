{
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "sessionKeepAliveTimeout": 30
  },
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Importing Relevant Libraries\n",
        ""
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "outputs": [],
      "metadata": {},
      "source": [
        "import json\n",
        "import time\n",
        "import requests\n",
        "import os\n",
        "from azure.storage.blob import BlockBlobService\n",
        "import pprint\n",
        "from os import listdir\n",
        "from os.path import isfile, join\n",
        "import shutil\n",
        "import pickle"
      ],
      "attachments": {}
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Create Local Folders\n",
        ""
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "outputs": [],
      "metadata": {},
      "source": [
        "# Create local directories if they don't exist\n",
        "# *mfg_source* contains all the pdf files to be converted to json\n",
        "if (not os.path.isdir(os.getcwd()+\"/form-datasets\")):\n",
        "    os.makedirs(os.getcwd()+\"/form-datasets\")\n",
        "# *formrecogoutput* will contain all the converted json files\n",
        "if (not os.path.isdir(os.getcwd()+\"/formrecogoutput\")):\n",
        "    os.makedirs(os.getcwd()+\"/formrecogoutput\")"
      ],
      "attachments": {}
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Downloading the PDF form from a cotainer\n",
        "\n",
        "Downloads all PDF from a container\n",
        "Does not download PDF which are already downloaded"
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "outputs": [
        {
          "execution_count": 6,
          "output_type": "execute_result",
          "data": {
            "text/plain": "CPU times: user 46.9 ms, sys: 3.82 ms, total: 50.7 ms\nWall time: 269 ms"
          },
          "metadata": {}
        }
      ],
      "metadata": {},
      "source": [
        "# %%time\n",
        "# Downloading pdf files from a container named *form-dataset* to a local folder *form-dataset**\n",
        "# Set up configs for blob storage\n",
        "STORAGE_ACCOUNT_NAME = \"#STORAGE_ACCOUNT_NAME#\"\n",
        "STORAGE_ACCOUNT_ACCESS_KEY = \"#STORAGE_ACCOUNT_KEY#\"\n",
        "STORAGE_CONTAINER_NAME = \"form-datasets\"\n",
        "\n",
        "# Instantiating a blob service object\n",
        "blob_service = BlockBlobService(STORAGE_ACCOUNT_NAME, STORAGE_ACCOUNT_ACCESS_KEY) \n",
        "\n",
        "blobs = blob_service.list_blobs(STORAGE_CONTAINER_NAME)\n",
        "# Downloading pdf files from the container *form-dataset** and storing them locally to *form-dataset** folder\n",
        "for blob in blobs:\n",
        "    # Check if the blob.name is already present in the folder form-dataset*. If yes then continue\n",
        "    if not blob.name.rsplit('.',1)[-1] == 'pdf':\n",
        "        continue\n",
        "    try:\n",
        "        with open('merged_log','rb') as f:\n",
        "            merged_files = pickle.load(f)\n",
        "    except FileNotFoundError:\n",
        "        merged_files = set()\n",
        "    # If file is already processed then continue to next file\n",
        "    if (blob.name in merged_files): \n",
        "        continue\n",
        "    download_file_path = os.path.join(os.getcwd(), \"form-datasets\", blob.name)\n",
        "    blob_service.get_blob_to_path(STORAGE_CONTAINER_NAME, blob.name ,download_file_path)\n",
        "    merged_files.add(blob.name)\n",
        "    # Keep trace of all the processed files at the end of your script (to keep track later)\n",
        "    with open('merged_log', 'wb') as f:\n",
        "        pickle.dump(merged_files, f)"
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "outputs": [
        {
          "execution_count": 7,
          "output_type": "execute_result",
          "data": {
            "text/plain": "5"
          },
          "metadata": {}
        }
      ],
      "metadata": {},
      "source": [
        "# Total number of forms to be converted to JSON\n",
        "files = [f for f in listdir(os.getcwd()+\"/form-datasets\") if isfile(join(os.getcwd()+\"/form-datasets\", f))]\n",
        "len(files)"
      ],
      "attachments": {}
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Querying the custom form recognizer model (PDF -> JSON)\n",
        "?\n",
        "Converts PDF -> JSON by querying the trained custom model.\n",
        "Clean the JSON file\n",
        "If a file has already been converted to JSON then skip it.## Cell title\n",
        ""
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "outputs": [
        {
          "execution_count": 8,
          "output_type": "execute_result",
          "data": {
            "text/plain": "POST analyze succeeded:\n{'Content-Length': '0', 'Operation-Location': 'https://westus2.api.cognitive.microsoft.com/formrecognizer/v2.0/custom/models/543aa8a8-639f-436d-bc0a-eecf12d99f81/analyzeresults/313c2338-3876-448d-9c1f-fb5cc0bfd1b8', 'x-envoy-upstream-service-time': '73', 'apim-request-id': 'f4dd6609-053c-4c4e-974a-8759be0bfe1e', 'Strict-Transport-Security': 'max-age=31536000; includeSubDomains; preload', 'x-content-type-options': 'nosniff', 'Date': 'Fri, 18 Sep 2020 14:55:38 GMT'}\nAnalysis succeeded:\n202045001\nPOST analyze succeeded:\n{'Content-Length': '0', 'Operation-Location': 'https://westus2.api.cognitive.microsoft.com/formrecognizer/v2.0/custom/models/543aa8a8-639f-436d-bc0a-eecf12d99f81/analyzeresults/bbcf976b-8ebc-42b4-82ff-1e4a2a303553', 'x-envoy-upstream-service-time': '54', 'apim-request-id': 'eb874040-6550-4e8b-8da0-d919bf68d38e', 'Strict-Transport-Security': 'max-age=31536000; includeSubDomains; preload', 'x-content-type-options': 'nosniff', 'Date': 'Fri, 18 Sep 2020 14:55:53 GMT'}\nAnalysis succeeded:\n202045000\nPOST analyze succeeded:\n{'Content-Length': '0', 'Operation-Location': 'https://westus2.api.cognitive.microsoft.com/formrecognizer/v2.0/custom/models/543aa8a8-639f-436d-bc0a-eecf12d99f81/analyzeresults/4e22ec84-0e26-4a49-9ab2-ec5b757b784c', 'x-envoy-upstream-service-time': '64', 'apim-request-id': 'fb12b626-50bd-429f-ab6f-5a671fd70660', 'Strict-Transport-Security': 'max-age=31536000; includeSubDomains; preload', 'x-content-type-options': 'nosniff', 'Date': 'Fri, 18 Sep 2020 14:56:08 GMT'}\nAnalysis succeeded:\n202045020\nPOST analyze succeeded:\n{'Content-Length': '0', 'Operation-Location': 'https://westus2.api.cognitive.microsoft.com/formrecognizer/v2.0/custom/models/543aa8a8-639f-436d-bc0a-eecf12d99f81/analyzeresults/11b6c739-2257-47e0-b83f-fb4fd6e29c39', 'x-envoy-upstream-service-time': '63', 'apim-request-id': 'd00baf52-54fd-4805-bd85-1bde1dfd2dda', 'Strict-Transport-Security': 'max-age=31536000; includeSubDomains; preload', 'x-content-type-options': 'nosniff', 'Date': 'Fri, 18 Sep 2020 14:56:23 GMT'}\nAnalysis succeeded:\n202045016\nPOST analyze succeeded:\n{'Content-Length': '0', 'Operation-Location': 'https://westus2.api.cognitive.microsoft.com/formrecognizer/v2.0/custom/models/543aa8a8-639f-436d-bc0a-eecf12d99f81/analyzeresults/8f8df90d-ad1c-4dc2-9564-e72c5979a078', 'x-envoy-upstream-service-time': '67', 'apim-request-id': 'c0fed919-6529-4953-a1d0-b86cf1085258', 'Strict-Transport-Security': 'max-age=31536000; includeSubDomains; preload', 'x-content-type-options': 'nosniff', 'Date': 'Fri, 18 Sep 2020 14:56:39 GMT'}\nAnalysis succeeded:\n202045005\nCPU times: user 398 ms, sys: 7.79 ms, total: 406 ms\nWall time: 1min 16s"
          },
          "metadata": {}
        }
      ],
      "metadata": {},
      "source": [
        "# %%time\n",
        "# Endpoint parameters for querying the custom trained form-recognizer model to return the processed JSON\n",
        "# Processes PDF files one by one and return CLEAN JSON files\n",
        "endpoint = r\"https://#LOCATION#.api.cognitive.microsoft.com/\"\n",
        "# Change if api key is expired\n",
        "apim_key = \"#APIM_KEY#\"\n",
        "# This model is the one trained on 5 forms\n",
        "model_id = \"#MODEL_ID#\"\n",
        "post_url = endpoint + r\"/formrecognizer/v2.0/custom/models/%s/analyze\" % model_id\n",
        "params = {\"includeTextDetails\": True}\n",
        "headers = {'Content-Type': 'application/pdf', 'Ocp-Apim-Subscription-Key': apim_key}\n",
        "\n",
        "local_path = os.path.join(os.getcwd(), \"form-datasets//\")\n",
        "output_path = os.path.join(os.getcwd(), \"formrecogoutput//\")\n",
        "files = [f for f in listdir(local_path) if isfile(join(local_path, f))]\n",
        "\n",
        "for file in files:\n",
        "    if not file.rsplit('.',1)[-1] == 'pdf':\n",
        "        continue\n",
        "    try:\n",
        "        with open('json_log','rb') as l:\n",
        "            json_files = pickle.load(l)\n",
        "    except FileNotFoundError:\n",
        "        json_files = set()\n",
        "    if (file in json_files): \n",
        "        continue\n",
        "    else:\n",
        "        with open(join(local_path,file), \"rb\") as f:\n",
        "            data_bytes = f.read()\n",
        "        \n",
        "    try:\n",
        "        resp = requests.post(url = post_url, data = data_bytes, headers = headers, params = params)\n",
        "        if resp.status_code != 202:\n",
        "            print(\"POST analyze failed:\\n%s\" % json.dumps(resp.json()))\n",
        "            #quit()\n",
        "        else:\n",
        "            print(\"POST analyze succeeded:\\n%s\" % resp.headers)\n",
        "            get_url = resp.headers[\"operation-location\"]\n",
        "    except Exception as e:\n",
        "        print(\"POST analyze failed:\\n%s\" % str(e))\n",
        "        #quit()\n",
        "     \n",
        "    n_tries = 15\n",
        "    n_try = 0\n",
        "    wait_sec = 5\n",
        "    max_wait_sec = 60\n",
        "    while n_try < n_tries:\n",
        "        try:\n",
        "            resp = requests.get(url = get_url, headers = {\"Ocp-Apim-Subscription-Key\": apim_key})\n",
        "            resp_json = resp.json()\n",
        "            if resp.status_code != 200:\n",
        "                print(\"GET analyze results failed:\\n%s\" % json.dumps(resp_json))\n",
        "                #quit()\n",
        "            status = resp_json[\"status\"]\n",
        "            if status == \"succeeded\":\n",
        "                print(\"Analysis succeeded:\\n%s\" % file[:-4])\n",
        "                allkeys = resp_json['analyzeResult']['documentResults'][0]['fields'].keys()\n",
        "                new_dict = {}\n",
        "                for i in allkeys:\n",
        "                    if resp_json['analyzeResult']['documentResults'][0]['fields'][i] != None:\n",
        "                        key = i.replace(\" \", \"_\")\n",
        "                        new_dict[key] = resp_json['analyzeResult']['documentResults'][0]['fields'][i]['valueString']\n",
        "                    else:\n",
        "                        key = i.replace(\" \", \"_\")\n",
        "                        new_dict[key] = None\n",
        "                # Appending form url to json\n",
        "                new_dict['form_url'] = 'https://#STORAGE_ACCOUNT_NAME#.blob.core.windows.net/form-datasets/' + file \n",
        "                with open(join(output_path,file[:-4]+\".json\"), 'w') as outfile:\n",
        "                    json.dump(new_dict, outfile)\n",
        "                # Change the encoding of file in case of spanish forms. It will detected random characters\n",
        "                with open(join(output_path,file[:-4]+\".json\"), 'w', encoding='utf-8') as outfile:\n",
        "                    json.dump(new_dict, outfile, ensure_ascii=False)\n",
        "                # Once JSON is saved log it otherwise don't log it.\n",
        "                json_files.add(file)\n",
        "                with open('json_log', 'wb') as f:\n",
        "                    pickle.dump(json_files, f)\n",
        "\n",
        "                break\n",
        "            if status == \"failed\":\n",
        "                print(\"Analysis failed:\\n%s\" % json.dumps(resp_json))\n",
        "                #quit()\n",
        "            # Analysis still running. Wait and retry.\n",
        "            time.sleep(wait_sec)\n",
        "            n_try += 1\n",
        "            wait_sec = min(2*wait_sec, max_wait_sec)     \n",
        "        except Exception as e:\n",
        "            msg = \"GET analyze results failed:\\n%s\" % str(e)\n",
        "            print(msg)\n",
        "            #quit()"
      ],
      "attachments": {}
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Upload the JSON files to a cotainer## Cell title\n",
        ""
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "outputs": [
        {
          "execution_count": 9,
          "output_type": "execute_result",
          "data": {
            "text/plain": "5"
          },
          "metadata": {}
        }
      ],
      "metadata": {},
      "source": [
        "# Total number of converted JSON\n",
        "files = [f for f in listdir(output_path) if isfile(join(output_path, f))]\n",
        "len(files)"
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "outputs": [
        {
          "execution_count": 10,
          "output_type": "execute_result",
          "data": {
            "text/plain": "CPU times: user 294 us, sys: 47 us, total: 341 us\nWall time: 350 us"
          },
          "metadata": {}
        }
      ],
      "metadata": {},
      "source": [
        "# %%time\n",
        "# Connect to the container for uploading the JSON files\n",
        "# Set up configs for blob storage\n",
        "STORAGE_ACCOUNT_NAME = \"#STORAGE_ACCOUNT_NAME#\"\n",
        "STORAGE_ACCOUNT_ACCESS_KEY = \"#STORAGE_ACCOUNT_KEY#\"\n",
        "# Upload the JSON files in this container\n",
        "STORAGE_CONTAINER_NAME = \"formrecogoutput\"\n",
        "# Instantiating a blob service object\n",
        "blob_service = BlockBlobService(STORAGE_ACCOUNT_NAME, STORAGE_ACCOUNT_ACCESS_KEY)"
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "outputs": [
        {
          "execution_count": 11,
          "output_type": "execute_result",
          "data": {
            "text/plain": "/mnt/var/hadoop/tmp/nm-local-dir/usercache/trusted-service-user/appcache/application_1600429513143_0014/container_1600429513143_0014_01_000001/formrecogoutput\n/mnt/var/hadoop/tmp/nm-local-dir/usercache/trusted-service-user/appcache/application_1600429513143_0014/container_1600429513143_0014_01_000001/formrecogoutput/202045001.json\n/mnt/var/hadoop/tmp/nm-local-dir/usercache/trusted-service-user/appcache/application_1600429513143_0014/container_1600429513143_0014_01_000001/formrecogoutput/202045016.json\n/mnt/var/hadoop/tmp/nm-local-dir/usercache/trusted-service-user/appcache/application_1600429513143_0014/container_1600429513143_0014_01_000001/formrecogoutput/202045000.json\n/mnt/var/hadoop/tmp/nm-local-dir/usercache/trusted-service-user/appcache/application_1600429513143_0014/container_1600429513143_0014_01_000001/formrecogoutput/202045005.json\n/mnt/var/hadoop/tmp/nm-local-dir/usercache/trusted-service-user/appcache/application_1600429513143_0014/container_1600429513143_0014_01_000001/formrecogoutput/202045020.json\nCPU times: user 33 ms, sys: 0 ns, total: 33 ms\nWall time: 232 ms"
          },
          "metadata": {}
        }
      ],
      "metadata": {},
      "source": [
        "# %%time\n",
        "# Upload JSON files from local folder *formrecogoutput* to the container *formrecogoutput*\n",
        "local_path = os.path.join(os.getcwd(), \"formrecogoutput\")\n",
        "print(local_path)\n",
        "for files in os.listdir(local_path):\n",
        "    print(os.path.join(local_path,files))\n",
        "    blob_service.create_blob_from_path(STORAGE_CONTAINER_NAME, files, os.path.join(local_path,files))"
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "outputs": [],
      "metadata": {},
      "source": [
        ""
      ],
      "attachments": {}
    }
  ]
}
