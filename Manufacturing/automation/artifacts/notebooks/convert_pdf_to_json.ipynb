{
  "metadata": {
    "saveOutput": true,
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Importing Relevant Libraries\n",
        ""
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "outputs": [],
      "metadata": {},
      "source": [
        "import json\n",
        "import time\n",
        "import requests\n",
        "import os\n",
        "from azure.storage.blob import BlockBlobService\n",
        "import pprint\n",
        "from os import listdir\n",
        "from os.path import isfile, join\n",
        "import shutil\n",
        "import pickle"
      ],
      "attachments": {}
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Create Local Folders\n",
        ""
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "outputs": [],
      "metadata": {},
      "source": [
        "# Create local directories if they don't exist\n",
        "# *mfg_source* contains all the pdf files to be converted to json\n",
        "if (not os.path.isdir(os.getcwd()+\"/incidentreport\")):\n",
        "    os.makedirs(os.getcwd()+\"/incidentreport\")\n",
        "# *formrecogoutput* will contain all the converted json files\n",
        "if (not os.path.isdir(os.getcwd()+\"/formrecogoutput\")):\n",
        "    os.makedirs(os.getcwd()+\"/formrecogoutput\")"
      ],
      "attachments": {}
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Downloading the PDF form from a cotainer\n",
        "\n",
        "Downloads all PDF from a container\n",
        "Does not download PDF which are already downloaded"
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 6,
          "data": {
            "text/plain": "<azure.storage.blob.models.Blob object at 0x7ff81cfd6b00>"
          },
          "metadata": {}
        }
      ],
      "metadata": {},
      "source": [
        "# %%time\n",
        "# Downloading pdf files from a container named *form-dataset* to a local folder *form-dataset**\n",
        "# Set up configs for blob storage\n",
        "STORAGE_ACCOUNT_NAME = \"#STORAGE_ACCOUNT_NAME#\"\n",
        "STORAGE_ACCOUNT_ACCESS_KEY = \"#STORAGE_ACCOUNT_KEY#\"\n",
        "STORAGE_CONTAINER_NAME = \"incidentreport\"\n",
        "STORAGE_LOG_CONTAINER=\"formrecoglog\"\n",
        "\n",
        "# Instantiating a blob service object\n",
        "blob_service = BlockBlobService(STORAGE_ACCOUNT_NAME, STORAGE_ACCOUNT_ACCESS_KEY) \n",
        "\n",
        "# Download the log files used for tracking the processing of forms\n",
        "log_blobs = blob_service.list_blobs(STORAGE_LOG_CONTAINER)\n",
        "for blob in log_blobs:\n",
        "    blob_service.get_blob_to_path(STORAGE_LOG_CONTAINER,blob.name,os.path.join(os.getcwd(), blob.name))\n",
        "\n",
        "blobs = blob_service.list_blobs(STORAGE_CONTAINER_NAME)\n",
        "# Downloading pdf files from the container *form-dataset** and storing them locally to *form-dataset** folder\n",
        "for blob in blobs:\n",
        "    # Check if the blob.name is already present in the folder form-dataset*. If yes then continue\n",
        "    if not blob.name.rsplit('.',1)[-1] == 'pdf':\n",
        "        continue\n",
        "    try:\n",
        "        with open('merged_log','rb') as f:\n",
        "            merged_files = pickle.load(f)\n",
        "    except FileNotFoundError:\n",
        "        merged_files = set()\n",
        "    # If file is already processed then continue to next file\n",
        "    if (blob.name in merged_files): \n",
        "        continue\n",
        "    download_file_path = os.path.join(os.getcwd(), \"incidentreport\", blob.name)\n",
        "    blob_service.get_blob_to_path(STORAGE_CONTAINER_NAME, blob.name ,download_file_path)\n",
        "    merged_files.add(blob.name)\n",
        "    # Keep trace of all the processed files at the end of your script (to keep track later)\n",
        "    with open('merged_log', 'wb') as f:\n",
        "        pickle.dump(merged_files, f)"
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 7,
          "data": {
            "text/plain": "1"
          },
          "metadata": {}
        }
      ],
      "metadata": {},
      "source": [
        "# Total number of forms to be converted to JSON\n",
        "files = [f for f in listdir(os.getcwd()+\"/incidentreport\") if isfile(join(os.getcwd()+\"/incidentreport\", f))]\n",
        "len(files)"
      ],
      "attachments": {}
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Querying the custom form recognizer model (PDF -> JSON)\n",
        "?\n",
        "Converts PDF -> JSON by querying the trained custom model.\n",
        "Clean the JSON file\n",
        "If a file has already been converted to JSON then skip it.## Cell title\n",
        ""
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 8,
          "data": {
            "text/plain": "POST analyze succeeded:\n{'Content-Length': '0', 'Operation-Location': 'https://westus2.api.cognitive.microsoft.com/formrecognizer/v2.0/custom/models/a002139f-7f8f-403b-b864-6ecfc692c16f/analyzeresults/18d84eee-e167-4644-929d-1da0728287f9', 'x-envoy-upstream-service-time': '62', 'apim-request-id': '88f06ed2-2edb-482f-b6f8-bdbd6cd2e302', 'Strict-Transport-Security': 'max-age=31536000; includeSubDomains; preload', 'x-content-type-options': 'nosniff', 'Date': 'Fri, 25 Sep 2020 23:29:47 GMT'}\nAnalysis succeeded:\n212045000"
          },
          "metadata": {}
        }
      ],
      "metadata": {},
      "source": [
        "# %%time\n",
        "# Endpoint parameters for querying the custom trained form-recognizer model to return the processed JSON\n",
        "# Processes PDF files one by one and return CLEAN JSON files\n",
        "endpoint = r\"https://#LOCATION#.api.cognitive.microsoft.com/\"\n",
        "# Change if api key is expired\n",
        "apim_key = \"#APIM_KEY#\"\n",
        "# This model is the one trained on 5 forms\n",
        "model_id = \"#MODEL_ID#\"\n",
        "post_url = endpoint + r\"/formrecognizer/v2.0/custom/models/%s/analyze\" % model_id\n",
        "params = {\"includeTextDetails\": True}\n",
        "headers = {'Content-Type': 'application/pdf', 'Ocp-Apim-Subscription-Key': apim_key}\n",
        "\n",
        "local_path = os.path.join(os.getcwd(), \"incidentreport//\")\n",
        "output_path = os.path.join(os.getcwd(), \"formrecogoutput//\")\n",
        "files = [f for f in listdir(local_path) if isfile(join(local_path, f))]\n",
        "\n",
        "for file in files:\n",
        "    if not file.rsplit('.',1)[-1] == 'pdf':\n",
        "        continue\n",
        "    try:\n",
        "        with open('json_log','rb') as l:\n",
        "            json_files = pickle.load(l)\n",
        "    except FileNotFoundError:\n",
        "        json_files = set()\n",
        "    if (file in json_files): \n",
        "        continue\n",
        "    else:\n",
        "        with open(join(local_path,file), \"rb\") as f:\n",
        "            data_bytes = f.read()\n",
        "        \n",
        "    try:\n",
        "        resp = requests.post(url = post_url, data = data_bytes, headers = headers, params = params)\n",
        "        if resp.status_code != 202:\n",
        "            print(\"POST analyze failed:\\n%s\" % json.dumps(resp.json()))\n",
        "            #quit()\n",
        "        else:\n",
        "            print(\"POST analyze succeeded:\\n%s\" % resp.headers)\n",
        "            get_url = resp.headers[\"operation-location\"]\n",
        "    except Exception as e:\n",
        "        print(\"POST analyze failed:\\n%s\" % str(e))\n",
        "        #quit()\n",
        "     \n",
        "    n_tries = 15\n",
        "    n_try = 0\n",
        "    wait_sec = 5\n",
        "    max_wait_sec = 60\n",
        "    while n_try < n_tries:\n",
        "        try:\n",
        "            resp = requests.get(url = get_url, headers = {\"Ocp-Apim-Subscription-Key\": apim_key})\n",
        "            resp_json = resp.json()\n",
        "            if resp.status_code != 200:\n",
        "                print(\"GET analyze results failed:\\n%s\" % json.dumps(resp_json))\n",
        "                #quit()\n",
        "            status = resp_json[\"status\"]\n",
        "            if status == \"succeeded\":\n",
        "                print(\"Analysis succeeded:\\n%s\" % file[:-4])\n",
        "                allkeys = resp_json['analyzeResult']['documentResults'][0]['fields'].keys()\n",
        "                new_dict = {}\n",
        "                for i in allkeys:\n",
        "                    if resp_json['analyzeResult']['documentResults'][0]['fields'][i] != None:\n",
        "                        key = i.replace(\" \", \"_\")\n",
        "                        new_dict[key] = resp_json['analyzeResult']['documentResults'][0]['fields'][i]['valueString']\n",
        "                    else:\n",
        "                        key = i.replace(\" \", \"_\")\n",
        "                        new_dict[key] = None\n",
        "                # Appending form url to json\n",
        "                new_dict['form_url'] = 'https://#STORAGE_ACCOUNT_NAME#.blob.core.windows.net/incidentreport/' + file \n",
        "                with open(join(output_path,file[:-4]+\".json\"), 'w') as outfile:\n",
        "                    json.dump(new_dict, outfile)\n",
        "                # Change the encoding of file in case of spanish forms. It will detected random characters\n",
        "                with open(join(output_path,file[:-4]+\".json\"), 'w', encoding='utf-8') as outfile:\n",
        "                    json.dump(new_dict, outfile, ensure_ascii=False)\n",
        "                # Once JSON is saved log it otherwise don't log it.\n",
        "                json_files.add(file)\n",
        "                with open('json_log', 'wb') as f:\n",
        "                    pickle.dump(json_files, f)\n",
        "\n",
        "                break\n",
        "            if status == \"failed\":\n",
        "                print(\"Analysis failed:\\n%s\" % json.dumps(resp_json))\n",
        "                #quit()\n",
        "            # Analysis still running. Wait and retry.\n",
        "            time.sleep(wait_sec)\n",
        "            n_try += 1\n",
        "            wait_sec = min(2*wait_sec, max_wait_sec)     \n",
        "        except Exception as e:\n",
        "            msg = \"GET analyze results failed:\\n%s\" % str(e)\n",
        "            print(msg)\n",
        "            #quit()"
      ],
      "attachments": {}
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Upload the JSON files to a cotainer## Cell title\n",
        ""
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 9,
          "data": {
            "text/plain": "1"
          },
          "metadata": {}
        }
      ],
      "metadata": {},
      "source": [
        "# Total number of converted JSON\n",
        "files = [f for f in listdir(output_path) if isfile(join(output_path, f))]\n",
        "len(files)"
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "outputs": [],
      "metadata": {},
      "source": [
        "# %%time\n",
        "# Connect to the container for uploading the JSON files\n",
        "# Set up configs for blob storage\n",
        "STORAGE_ACCOUNT_NAME = \"#STORAGE_ACCOUNT_NAME#\"\n",
        "STORAGE_ACCOUNT_ACCESS_KEY = \"#STORAGE_ACCOUNT_KEY#\"\n",
        "# Upload the JSON files in this container\n",
        "STORAGE_CONTAINER_NAME = \"formrecogoutput\"\n",
        "# Instantiating a blob service object\n",
        "blob_service = BlockBlobService(STORAGE_ACCOUNT_NAME, STORAGE_ACCOUNT_ACCESS_KEY)"
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 11,
          "data": {
            "text/plain": "/mnt/var/hadoop/tmp/nm-local-dir/usercache/trusted-service-user/appcache/application_1601072998832_0005/container_1601072998832_0005_01_000001/formrecogoutput\n/mnt/var/hadoop/tmp/nm-local-dir/usercache/trusted-service-user/appcache/application_1601072998832_0005/container_1601072998832_0005_01_000001/formrecogoutput/212045000.json\n<azure.storage.blob.models.ResourceProperties object at 0x7ff81c8c7828>"
          },
          "metadata": {}
        }
      ],
      "metadata": {},
      "source": [
        "# %%time\n",
        "# Upload JSON files from local folder *formrecogoutput* to the container *formrecogoutput*\n",
        "local_path = os.path.join(os.getcwd(), \"formrecogoutput\")\n",
        "print(local_path)\n",
        "for files in os.listdir(local_path):\n",
        "    print(os.path.join(local_path,files))\n",
        "    blob_service.create_blob_from_path(STORAGE_CONTAINER_NAME, files, os.path.join(local_path,files))"
      ],
      "attachments": {}
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 12,
          "data": {
            "text/plain": "<azure.storage.blob.models.ResourceProperties object at 0x7ff81c8c7588>"
          },
          "metadata": {}
        }
      ],
      "metadata": {},
      "source": [
        "# Upload the log files used for tracking form processing to blob container\n",
        "blob_service.create_blob_from_path(STORAGE_LOG_CONTAINER,\"merged_log\",os.path.join(os.getcwd(), \"merged_log\"))\n",
        "blob_service.create_blob_from_path(STORAGE_LOG_CONTAINER,\"json_log\",os.path.join(os.getcwd(), \"json_log\"))"
      ],
      "attachments": {}
    }
  ]
}