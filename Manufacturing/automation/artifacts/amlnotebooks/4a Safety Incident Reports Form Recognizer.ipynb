{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Azure Form Recognizer\n",
    "\n",
    "Azure Form Recognizer is a cognitive service that uses machine learning technology to identify and extract key-value pairs and table data from form documents. It then outputs structured data that includes the relationships in the original file.\n",
    "\n",
    "    \n",
    "![](https://dreamdemostorageforgen2.blob.core.windows.net/mfgdemodata/Incident_Reports.jpg)\n",
    "\n",
    "### Overview\n",
    "*Safety Incident Reports Dataset*: Raw unstructured data is fed into the pipeline in the form of electronically generated PDFs. These reports contain information about injuries that occurred at 5 different factories belonging to a company. This data provides information on injury reports, including the nature, description, date, source and the name of the establishment where it happened. \n",
    "\n",
    "\n",
    "### Notebook Organization \n",
    "+ Fetch the injury report PDF files from a container under an azure storage account.\n",
    "\n",
    "+ Convert the PDF files to JSON by querying the azure trained form recognizer model using the REST API.\n",
    "\n",
    "+ Preprocess the JSON files to extract only relevant information.\n",
    "\n",
    "+ Push the JSON files to a container under an azure storage account."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Relevant Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "gather": {
     "logged": 1600844220921
    }
   },
   "outputs": [],
   "source": [
    "# Please install this specific version of azure storage blob compatible with this notebook.\n",
    "!pip install --quiet azure-storage-blob==2.1.0\n",
    "\n",
    "# Import the required libraries\n",
    "import json\n",
    "import time\n",
    "import requests\n",
    "import os\n",
    "from azure.storage.blob import BlockBlobService\n",
    "import pprint\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import shutil\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Local Folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "gather": {
     "logged": 1600844226889
    }
   },
   "outputs": [],
   "source": [
    "# Create local directories if they don't exist\n",
    "# *input_forms* contains all the pdf files to be converted to json\n",
    "if (not os.path.isdir(os.getcwd()+\"/input_forms\")):\n",
    "    os.makedirs(os.getcwd()+\"/input_forms\")\n",
    "# *output_json* will contain all the converted json files\n",
    "if (not os.path.isdir(os.getcwd()+\"/output_json\")):\n",
    "    os.makedirs(os.getcwd()+\"/output_json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloading the PDF forms from a container in azure storage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Downloads all PDF forms from a container named *incidentreport* to a local folder *input_forms*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 23.7 ms, sys: 20.1 ms, total: 43.8 ms\n",
      "Wall time: 915 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Downloading pdf files from a container named *incidentreport* to a local folder *input_forms*\n",
    "# Importing user defined config\n",
    "import config\n",
    "\n",
    "# setting up blob storage configs\n",
    "STORAGE_ACCOUNT_NAME = config.STORAGE_ACCOUNT_NAME\n",
    "STORAGE_ACCOUNT_ACCESS_KEY = config.STORAGE_ACCOUNT_ACCESS_KEY\n",
    "STORAGE_CONTAINER_NAME = \"form-datasets\"\n",
    "\n",
    "# Instantiating a blob service object\n",
    "blob_service = BlockBlobService(STORAGE_ACCOUNT_NAME, STORAGE_ACCOUNT_ACCESS_KEY) \n",
    "\n",
    "blobs = blob_service.list_blobs(STORAGE_CONTAINER_NAME)\n",
    "# Downloading pdf files from the container *incidentreport* and storing them locally to *input_forms* folder\n",
    "for blob in blobs:\n",
    "    if not blob.name.rsplit('.',1)[-1] == 'pdf':\n",
    "        continue\n",
    "    # Check if the blob.name is already present in the folder input_forms. If yes then continue\n",
    "    try:\n",
    "        with open('merged_log','rb') as f:\n",
    "            merged_files = pickle.load(f)\n",
    "    except FileNotFoundError:\n",
    "        merged_files = set()\n",
    "    # If file is already processed then continue to next file\n",
    "    if (blob.name in merged_files):\n",
    "        print(blob.name)\n",
    "        continue\n",
    "    download_file_path = os.path.join(os.getcwd(), \"input_forms\", blob.name)\n",
    "    blob_service.get_blob_to_path(STORAGE_CONTAINER_NAME, blob.name ,download_file_path)\n",
    "    merged_files.add(blob.name)\n",
    "    # Keep trace of all the processed files at the end of your script (to keep track later)\n",
    "    with open('merged_log', 'wb') as f:\n",
    "        pickle.dump(merged_files, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "gather": {
     "logged": 1600844418895
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['202045000.pdf',\n",
       " '202045001.pdf',\n",
       " '202045005.pdf',\n",
       " '202045016.pdf',\n",
       " '202045020.pdf']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Total number of forms to be converted to JSON\n",
    "files = [f for f in listdir(os.getcwd()+\"/input_forms\") if isfile(join(os.getcwd()+\"/input_forms\", f))]\n",
    "files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Querying the custom trained form recognizer model (PDF -> JSON)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Converts PDF -> JSON by querying the trained custom model.\n",
    "- Preprocess the JSON file and extract only the relevant information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://westus2.api.cognitive.microsoft.com\n",
      "8535af565d934195b54dee076dd0aa3d\n",
      "d35340be-1522-46cb-b72a-ead573d0475b\n",
      "https://westus2.api.cognitive.microsoft.com/formrecognizer/v2.0/custom/models/d35340be-1522-46cb-b72a-ead573d0475b/analyze\n",
      "resp <Response [202]>\n",
      "POST analyze succeeded:\n",
      "{'Content-Length': '0', 'Operation-Location': 'https://westus2.api.cognitive.microsoft.com/formrecognizer/v2.0/custom/models/d35340be-1522-46cb-b72a-ead573d0475b/analyzeresults/0256fc12-b004-4a84-ad2e-08841fdcbd4f', 'x-envoy-upstream-service-time': '69', 'apim-request-id': 'ed60a913-8f6f-4e87-b221-8cdbffe7d24b', 'Strict-Transport-Security': 'max-age=31536000; includeSubDomains; preload', 'x-content-type-options': 'nosniff', 'Date': 'Wed, 23 Sep 2020 07:34:11 GMT'}\n",
      "Analysis succeeded:\n",
      "202045000\n",
      "resp <Response [202]>\n",
      "POST analyze succeeded:\n",
      "{'Content-Length': '0', 'Operation-Location': 'https://westus2.api.cognitive.microsoft.com/formrecognizer/v2.0/custom/models/d35340be-1522-46cb-b72a-ead573d0475b/analyzeresults/41b8a0dc-b801-45e4-9fd8-1cf9e17885ad', 'x-envoy-upstream-service-time': '62', 'apim-request-id': 'eb5fa97e-e95a-49e5-bbfb-62a1a0d703e8', 'Strict-Transport-Security': 'max-age=31536000; includeSubDomains; preload', 'x-content-type-options': 'nosniff', 'Date': 'Wed, 23 Sep 2020 07:34:26 GMT'}\n",
      "Analysis succeeded:\n",
      "202045001\n",
      "resp <Response [202]>\n",
      "POST analyze succeeded:\n",
      "{'Content-Length': '0', 'Operation-Location': 'https://westus2.api.cognitive.microsoft.com/formrecognizer/v2.0/custom/models/d35340be-1522-46cb-b72a-ead573d0475b/analyzeresults/db61bf1b-7f20-49e1-a75b-470237f126d6', 'x-envoy-upstream-service-time': '87', 'apim-request-id': 'fe7ed8a2-e5ab-4488-8350-7788e601ea51', 'Strict-Transport-Security': 'max-age=31536000; includeSubDomains; preload', 'x-content-type-options': 'nosniff', 'Date': 'Wed, 23 Sep 2020 07:34:42 GMT'}\n",
      "Analysis succeeded:\n",
      "202045005\n",
      "resp <Response [202]>\n",
      "POST analyze succeeded:\n",
      "{'Content-Length': '0', 'Operation-Location': 'https://westus2.api.cognitive.microsoft.com/formrecognizer/v2.0/custom/models/d35340be-1522-46cb-b72a-ead573d0475b/analyzeresults/317a7d02-0fc5-4ba3-a073-548b3428b9d5', 'x-envoy-upstream-service-time': '57', 'apim-request-id': '13fda5e9-71ea-47f6-bc28-a9005e2fcc3b', 'Strict-Transport-Security': 'max-age=31536000; includeSubDomains; preload', 'x-content-type-options': 'nosniff', 'Date': 'Wed, 23 Sep 2020 07:34:57 GMT'}\n",
      "Analysis succeeded:\n",
      "202045016\n",
      "resp <Response [202]>\n",
      "POST analyze succeeded:\n",
      "{'Content-Length': '0', 'Operation-Location': 'https://westus2.api.cognitive.microsoft.com/formrecognizer/v2.0/custom/models/d35340be-1522-46cb-b72a-ead573d0475b/analyzeresults/0e64d489-b2fe-42d2-a41a-5b0a0a429f6e', 'x-envoy-upstream-service-time': '83', 'apim-request-id': '3172c2d8-065b-4e0a-b8be-d3a9dcd59b31', 'Strict-Transport-Security': 'max-age=31536000; includeSubDomains; preload', 'x-content-type-options': 'nosniff', 'Date': 'Wed, 23 Sep 2020 07:35:12 GMT'}\n",
      "Analysis succeeded:\n",
      "202045020\n",
      "CPU times: user 252 ms, sys: 35.6 ms, total: 287 ms\n",
      "Wall time: 1min 17s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Importing user defined config\n",
    "import config\n",
    "\n",
    "# Endpoint parameters for querying the custom trained form-recognizer model to return the processed JSON\n",
    "# Processes PDF files one by one and return CLEAN JSON files\n",
    "endpoint = config.FORM_RECOGNIZER_ENDPOINT\n",
    "print(endpoint)\n",
    "# Change if api key is expired\n",
    "apim_key = config.FORM_RECOGNIZER_APIM_KEY\n",
    "print(apim_key)\n",
    "# This model is the one trained on 5 forms\n",
    "model_id =config.FORM_RECOGNIZER_MODEL_ID\n",
    "print(model_id)\n",
    "post_url = endpoint + \"/formrecognizer/v2.0/custom/models/%s/analyze\" % model_id\n",
    "print(post_url)\n",
    "files = [f for f in listdir(os.getcwd()+\"/input_forms\") if isfile(join(os.getcwd()+\"/input_forms\", f))]\n",
    "params = {\"includeTextDetails\": True}\n",
    "headers = {'Content-Type': 'application/pdf', 'Ocp-Apim-Subscription-Key': apim_key}\n",
    "\n",
    "local_path = os.path.join(os.getcwd(), \"input_forms//\")\n",
    "output_path = os.path.join(os.getcwd(), \"output_json//\")\n",
    "\n",
    "\n",
    "\n",
    "for file in files:\n",
    "    try:\n",
    "        with open('json_log','rb') as l:\n",
    "            json_files = pickle.load(l)\n",
    "    except FileNotFoundError:\n",
    "        json_files = set()\n",
    "    if (file in json_files):\n",
    "        continue\n",
    "    else:\n",
    "        with open(local_path+file, \"rb\") as f:\n",
    "            data_bytes = f.read()\n",
    "    \n",
    "    try:\n",
    "        resp = requests.post(url = post_url, data = data_bytes, headers = headers, params = params)\n",
    "        print('resp',resp)\n",
    "        if resp.status_code != 202:\n",
    "            print(\"POST analyze failed:\\n%s\" % json.dumps(resp.json()))\n",
    "#             quit()\n",
    "            break\n",
    "        else:  \n",
    "            print(\"POST analyze succeeded:\\n%s\" % resp.headers)\n",
    "            get_url = resp.headers[\"operation-location\"]\n",
    "    except Exception as e:\n",
    "        print(\"POST analyze failed:\\n%s\" % str(e))\n",
    "#         quit()\n",
    "        break\n",
    "    n_tries = 15\n",
    "    n_try = 0\n",
    "    wait_sec = 5\n",
    "    max_wait_sec = 60\n",
    "    while n_try < n_tries:\n",
    "        try:\n",
    "            resp = requests.get(url = get_url, headers = {\"Ocp-Apim-Subscription-Key\": apim_key})\n",
    "            resp_json = resp.json()\n",
    "            if resp.status_code != 200:\n",
    "                print(\"GET analyze results failed:\\n%s\" % json.dumps(resp_json))\n",
    "                quit()\n",
    "            status = resp_json[\"status\"]\n",
    "            if status == \"succeeded\":\n",
    "                print(\"Analysis succeeded:\\n%s\" % file[:-4])\n",
    "                allkeys = resp_json['analyzeResult']['documentResults'][0]['fields'].keys()\n",
    "                new_dict = {}\n",
    "                for i in allkeys:\n",
    "                    if resp_json['analyzeResult']['documentResults'][0]['fields'][i] != None:\n",
    "                        key = i.replace(\" \", \"_\")\n",
    "                        new_dict[key] = resp_json['analyzeResult']['documentResults'][0]['fields'][i]['valueString']\n",
    "                    else:\n",
    "                        key = i.replace(\" \", \"_\")\n",
    "                        new_dict[key] = None\n",
    "                # Appending form url to json\n",
    "                new_dict['form_url'] = 'https://stcognitivesearch001.blob.core.windows.net/formupload/' + file \n",
    "                with open(output_path+file[:-4]+\".json\", 'w') as outfile:\n",
    "                    json.dump(new_dict, outfile)\n",
    "                # Change the encoding of file in case of spanish forms. It will detected random characters.\n",
    "                with open(output_path+file[:-4]+\".json\", 'w', encoding='utf-8') as outfile:\n",
    "                    json.dump(new_dict, outfile, ensure_ascii=False)\n",
    "                # Once JSON is saved log it otherwise don't log it.\n",
    "                json_files.add(file)\n",
    "                with open('json_log', 'wb') as f:\n",
    "                    pickle.dump(json_files, f)\n",
    "\n",
    "                break\n",
    "            if status == \"failed\":\n",
    "                print(\"Analysis failed:\\n%s\" % json.dumps(resp_json))\n",
    "#                 quit()\n",
    "            # Analysis still running. Wait and retry.\n",
    "            time.sleep(wait_sec)\n",
    "            n_try += 1\n",
    "            wait_sec = min(2*wait_sec, max_wait_sec)     \n",
    "        except Exception as e:\n",
    "            msg = \"GET analyze results failed:\\n%s\" % str(e)\n",
    "            print(msg)\n",
    "            # quit()\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload the JSON files to a cotainer\n",
    "\n",
    "- Upload JSON files from local folder *output_json* to the container *formrecogoutput*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Total number of converted JSON\n",
    "files = [f for f in listdir(os.getcwd()+\"/output_json\") if isfile(join(os.getcwd()+\"/output_json\", f))]\n",
    "len(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 158 µs, sys: 18 µs, total: 176 µs\n",
      "Wall time: 180 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Importing user defined config\n",
    "import config\n",
    "\n",
    "# Connect to the container for uploading the JSON files\n",
    "# Set up configs for blob storage\n",
    "STORAGE_ACCOUNT_NAME = config.STORAGE_ACCOUNT_NAME\n",
    "STORAGE_ACCOUNT_ACCESS_KEY = config.STORAGE_ACCOUNT_ACCESS_KEY\n",
    "# Upload the JSON files in this container\n",
    "STORAGE_CONTAINER_NAME = \"formrecogoutput\"\n",
    "# Instantiating a blob service object\n",
    "blob_service = BlockBlobService(STORAGE_ACCOUNT_NAME, STORAGE_ACCOUNT_ACCESS_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 19.6 ms, sys: 1.32 ms, total: 21 ms\n",
      "Wall time: 416 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Upload JSON files from local folder *output_json* to the container *formrecogoutput*\n",
    "local_path = os.path.join(os.getcwd(), \"output_json\")\n",
    "# print(local_path)\n",
    "for files in os.listdir(local_path):\n",
    "#     print(os.path.join(local_path,files))\n",
    "    blob_service.create_blob_from_path(STORAGE_CONTAINER_NAME, files, os.path.join(local_path,files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "python3"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
